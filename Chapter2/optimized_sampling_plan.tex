\section{Optimized Sampling Plans} \label{sec:sampling_plans}

The following section is concerned with the problem of identifying an optimal set of points at which to build a surrogate model for an expensive computer code when only $N$ evaluations can be afforded. All surrogate models are built around a set of points at which the objective computer code is actually evaluated. Intuitively, the surrogate accuracy is expected to decrease as one moves further away from such points. Consequently, it is important to spread $N$ points as uniformly as possible across the design space. 

\subsection{Morris Algorithm}
\label{subsec:morris_algorithm}

Expensive computer codes generally have many input parameters because they attempt to model some phenomena on a very fine scale as accurately as possible. Of course, when writing such computer codes engineers are not aware which input parameters have the greatest impact on the outputs of interest or else only these parameters would be modeled. Contrarily, engineers typically only know that certain parameters are involved in the pertinent physics in some way but not to what extent. Due to the curse of dimensionality, the less design variables considered in the construction of a surrogate the cheaper the computational cost will be \cite{Forrester}. Consequently, before attempting to construct a surrogate for an expensive computer code it is worth identifying which design variables are most active. After all, if a design variable has a trivial effect on some output of interest then its presence in the surrogate should be minimized, for this is the very purpose of a surrogate model. One method for weeding out unimportant design variables is described by Morris \cite{Morris}, which is summarized in algorithm \ref{code:morris_algorithm}.   

The premise of Morris' algorithm is that if the derivative of some output parameter with respect to a design variable changes significantly throughout the design space then the variable is important. If the output parameter does not change with respect to the design variable then the variable can safely be ignored. To this end, the metric in Eq. \ref{eq:elementary_effect_x} is introduced by Morris to estimate the so-called elementary effect $d_i\left(\textbf{x}\right)$ of design variable $x_i$.
\begin{equation}
\label{eq:elementary_effect_x}
   d_i\left(\textbf{x}\right) = \frac{f\left(x_1,x_2,...,x_{i-1},x_i+\Delta,x_{i+1},....,x_k 										\right) - f\left(\textbf{x}\right)}
   								{\Delta}      
\end{equation}  
In Eq. \ref{eq:elementary_effect_x}, $\Delta$ is a step-length size for the perturbation. For convenience all variables $x_i$ are normalized to unit length and divided into $p$ segments such that $x_i\in\lbrace 0,1/(p-1),2/(p-1),...,1\rbrace$. Choosing a set of $\textbf{x}$ carefully, it is possible to calculate an elementary effect for each of $k$ design variables using only $k+1$ function evaluations. Indeed, as described in \cite{Morris}, a $[k+1]\times [k]$ random orientation matrix $B^*$ can be constructed using the equation,
\begin{equation}
\label{eq:random_orientation}
   \textbf{B}^* = \left(\textbf{1}_{k+1,1}\textbf{x}^* + \frac{\Delta}{2}\left[
                   \left(2\textbf{B} - \textbf{1}_{k+1,k}\right)\textbf{D}^* + 
                    \textbf{1}_{k+1,k}\right]\right)\textbf{P}^*.
\end{equation}            
In Eq. \ref{eq:random_orientation}, $\textbf{1}$ is a matrix of ones with size specified by its subscript and $\textbf{B}$ is a $[k+1]\times [k]$ matrix of zeros and ones with the characteristic that for each column there exists a pair of rows differing in only their $i^{th}$ entry for $i\in\lbrace 1,2,...,k\rbrace$. Also, $\textbf{D}^*$ is a $[k]\times [k]$ diagonal matrix with ones of differing parity uniformly spread, $\textbf{P}^*$ is a $[k]\times [k]$ random permutation matrix, and $\textbf{x}^*$ is a random point chosen in the $p$-level design space. When the rows of $\textbf{B}^*$ are evaluated by the objective function and substituted into Eq. \ref{eq:elementary_effect_x} an elementary effect is calculated for each design variable. 

The more elementary effects that can be calculated for each design variable, the better the estimate as to the effect of each design variable on the objective function. Consequently, $r$ random orientation matrices are typically created to obtain a total of $r$ elementary effects for each design variable. Taking the mean and standard deviation of each variable's $r$ effects can yield insight into the most important variables. Plotting the mean and standard deviation of each variable's effects on a scatter plot, variables that have a negligible effect on the objective function will cluster around the origin. Large fluctuations in standard deviation are indicative of nonlinear and interactive effects \cite{Morris}. 

\begin{algorithm}
\caption{\label{code:morris_algorithm} 
Uses Morris' Algorithm to determine which of a function's design variables induce the most significant effects and interactions.} 
\begin{algorithmic}[1]
\State Initialize zeros matrix $d_{stats}$ of size $[r]\times[k]$
\For {$i=1:r$}
   \State $X\rightarrow$ Create random orientation matrix 
   \Comment{Eq. \ref{eq:random_orientation}} 
   \State $f_{base} = f(X[0,:])$
   \For {$j=1:k$}
      \State $f_{new} = f(X[k,:])$
      \State $l\rightarrow$ Find index of effect being perturbed
      \State $d_{effect}\rightarrow$ Calculate elementary effect \Comment{Eq. \ref{eq:elementary_effect_x}} 
      \State $d_{stats}[i,l] = d_{effect}$
      \State $f_{base} := f_{new}$
   \EndFor
\EndFor
\State Return mean and standard deviation of $d_{stats}$ 
\end{algorithmic}
\end{algorithm}

\subsection{Latin Hypercube Sampling}
\label{subsec:lhs}

One of the major problems with random sampling occurs when a relatively limited sample size is utilized. In this case, subsets of the sample space with high consequence but low probability are likely to be missed \cite{Helton}. In addition, the proximity of sampled values caused by random sampling is inefficient, which often causes slow convergence. In order to resolve such issues \ac{LHS} was conjured. The basis of \ac{LHS} rests upon dividing the normalized space of each design variable into $n$ equally sized bins if $n$ samples are required. As a result, when the $n$ samples are taken it is guaranteed that the entire spectrum of each design variable's space has been visited. Algorithmically an $n$ sample Latin hypercube in $k$ dimensions can be easily calculated, as described in algorithm \ref{code:lhs}.

\begin{algorithm}
\caption{\label{code:lhs} 
Creates a random Latin hypercube consisting of $n$ samples in $k$ dimensions.} 
\begin{algorithmic}[1]
\State Initialize zeros matrix $X$ of size $[n]\times[k]$
\For {$i=1:k$}
   \State $p\rightarrow$ Create random permutation of the set $\lbrace 1,2,...,n\rbrace$ 
   \State $X[:,i] := p[:]$
\EndFor
\State Map each entry of $X$ into hypercube
\State Return $X$ 
\end{algorithmic}
\end{algorithm}

Each row of the output from algorithm \ref{code:lhs} is a sample point normalized to a hypercube. As mentioned previously, when sampling a design space it is important to take sample points uniformly. While \ac{LHS} increases the likelihood of obtaining such a uniform sampling space at random it is actually possible to obtain an optimized sampling space based on the maximin metric \cite{Forrester}.

The maximin metric describe by Morris and Mitchell \cite{Morris_Mitchell} makes use of two notions in an attempt to quantify the uniformity, or 'space-fillingness', of a set of sampling points. In order to describe the notions for each sampling plan it is useful to gather $\lbrace d_1, d_2, ..., d_m\rbrace$ and $\lbrace J_1, J_2, ..., J_m\rbrace$, the unique distances between all points in the plan sorted in ascending order and the number of occurrences of each distance, respectively. In words, the Morris and Mitchell criteria states that an optimized sampling plan will minimize all $J_i$ while maximizing the corresponding $d_i$. More formally, Morris and Mitchell define the maximin sampling plan as one which maximizes $d_1$, and among plans for which this is true, minimizes $J_1$, among plans for which this is true, maximizes $d_2$, and so on \cite{Forrester}. The previous definition can be restated into pseudo-equivalent minimization problem by introducing the parameter $\Phi_q(\textbf{X})$,
\begin{equation}
\label{eq:Phi_q}
   \Phi_q(\textbf{X}) = \left(\sum_{j=1}^m J_j d_j^{-q} \right)^{1/q}
\end{equation}
where $\textbf{X}$ is a sampling plan and $q$ is a control parameter inherent in the minimization problem. 

The minimization of Eq. \ref{eq:Phi_q} and the Morris and Mitchell definition of the maximin sampling plan are generally used in unison to obtain a locally optimal sampling plan since finding the globally optimal plan is computationally infeasible. In this approach a random sampling plan $\textbf{X}_0$ is initially generated using algorithm \ref{code:lhs}. Using a range of $q$ values, usually from one to one-hundred, an optimal latin hypercube plan is found for each value based on the initial sampling plan $\textbf{X}_0$. To obtain an optimized plan for each $q$ algorithms such as simulated annealing \cite{Kirkpatrick} and evolutionary operation \cite{Box} can be used. These algorithms work to minimize Eq. \ref{eq:Phi_q} by comparing an initial sampling plan to perturbed versions, which are obtained by switching pairs of column entries in the output of algorithm \ref{code:lhs}. Once an optimized \ac{LHS} plan is found for each $q$ value, the resulting set of plans are contested directly against each other by explicit application of Morris and Mitchell's maximin definition. The sampling plan satisfying the maximin criteria is the locally optimal sampling plan to be used for proceeding surrogate model construction. Algorithm \ref{code:best_lhs} summarizes the search for a locally optimal \ac{LHS} plan.  

\begin{algorithm}
\caption{\label{code:best_lhs} 
Obtains a locally optimal \ac{LHS} plan using the Morris-Mitchell minimax criteria.} 
\begin{algorithmic}[1]
\State Initialize sampling plan $\textbf{X}_0$ \Comment{Apply algorithm \ref{code:lhs}}
\State $q = \left[1, 2, 5, 10, 20, 50, 100\right]$
\State $\textbf{X}^{cands.} \rightarrow$ Initialize empty array for optimal $\textbf{X}$ candidates
\For {$q_i$ in $q$}
   \State $\textbf{X}_{opt}(q_i)\rightarrow$ Find optimal $\textbf{X}$ for $q_i$ using simulated annealing \Comment{Minimize Eq. \ref{eq:Phi_q}}
   \State Add $\textbf{X}_{opt}(q_i)$ to $\textbf{X}^{cands.}$ 
\EndFor
\State Apply Morris-Mitchell criterion between all $(\textbf{X}^{cands.}_i, \textbf{X}^{cands.}_j)$ to find optimal plan
\end{algorithmic}
\end{algorithm}
  


 

