\section{Combining Decomposition and Smolyak's Algorithm} \label{sec:decomp_and_interp}

As hinted at in the beginning of chapter \ref{chap:rom}, the Smolyak algorithm combines with the anchored-\ac{ANOVA} decomposition to create a reduced order model for any well behaving computer code. To see how the Smolyak algorithm fits into the functional decomposition described in this chapter, begin by substituting \ref{eq:explicit_component_def} into \ref{eq:hdmr_decomp} to get,
\begin{equation} \label{eq:decomp_smol1}
   f(\textbf{x}) = 
    \sum_{\textbf{u}\subseteq\mathcal{D}}
     \sum_{\textbf{v}\subseteq\textbf{u}} 
      (-1)^{\vert\textbf{u}\vert - \vert\textbf{v}\vert}
       P_{\textbf{v}} f(\textbf{x}_{\textbf{v}}).
\end{equation} 
Now, insert the Dirac projection operator from \ref{eq:projection_operator_dirac} into \ref{eq:decomp_smol1} to arrive at,  
\begin{equation} \label{eq:decomp_smol2}
   f(\textbf{x}) = 
    \sum_{\textbf{u}\subseteq\mathcal{D}}
     \sum_{\textbf{v}\subseteq\textbf{u}} 
      (-1)^{\vert\textbf{u}\vert - \vert\textbf{v}\vert}
       f(\textbf{x})\vert_{  
        \textbf{x}=\textbf{a}\setminus\textbf{x}_{\textbf{v}} }.   
\end{equation}
To create a reduced order model the set $\mathcal{D}$ is ultimately shrunk to only contain a subset of all the variables of $f$ but this is discussed later. The important aspect of \ref{eq:decomp_smol2} to realize is that in order to evaluate $ f(\textbf{x}) \vert_{\textbf{x} = \textbf{a} \setminus \textbf{x}_{\textbf{v}}}$ evaluation of the expensive computer code $f(\textbf{x})$ is required. Consequently, the desireable property of reduced order models, that of rapid evaluation, is not achieved in \ref{eq:decomp_smol2}. The remedy is to approximate each $ f(\textbf{x}) \vert_{\textbf{x} = \textbf{a} \setminus \textbf{x}_{\textbf{v}}}$ using Smolyak interpolants. Substituting \ref{eq:Smolyak_formula2} into \ref{eq:decomp_smol2}, the formulation for creating a reduced order model of $f$ is complete. 
\begin{equation} \label{eq:decomp_smol3}
   f(\textbf{x}) = 
    \sum_{\textbf{u}\subseteq\mathcal{D}}
     \sum_{\textbf{v}\subseteq\textbf{u}} 
      (-1)^{\vert\textbf{u}\vert - \vert\textbf{v}\vert}
       \sum_{\vert\textbf{i}\vert\leq q}\left(
        \Delta^{i_1}\otimes\cdots\otimes\Delta^{i_{\vert\textbf{v}\vert}}
         \right)  
\end{equation}
While there is initial overhead to create an interpolant for each component in \ref{eq:decomp_smol2} the result is quick evaluation of the reduced order model. Details regarding the implementation and application of \ref{eq:decomp_smol3} will be discussed in the proceeding sections.               
 
\subsection{Implementation Details} \label{subsec:implement_details}

\subsubsection{Combinatorics Routines} \label{subsubsec:combinatorics_routines}

In order to implement \ref{eq:decomp_smol3} on a computer several enumeration routines need to be available. Unfortunately, these routines are not available in most numerical math libraries containing combinatorics routines. The first routine of interest solves the problem of how to enumerate all the ways $d$ positive integers can be summed to equal another integer. In other words, what are all the sets $\left\{i_1,...,i_d\right\}$ such that $i_1 + i_2 + ... + i_d = q$? This problem inserts itself in \ref{eq:decomp_smol3} in the summation index for Smolyak interpolation. As the Smolyak interpolant becomes refined from level to level---$q$ is increased by one in each refinement---the Smolyak algorithm must newly account for all $\textbf{i}$ such that $\vert\textbf{i}\vert=q$. The following enumeration algorithm, a slight modification of the original algorithm found in \cite{Holtz}, finds all the desired indice sets:        
\begin{algorithm}
\caption{\label{code:enumeration1} 
For postitive integers $d$ and $q$ this code outputs all sets $\left\{i_1,i_2,...,i_d\right\}$ such that $i_1+i_2+...+i_d=q$.} 
\begin{algorithmic}[1]
\State $p = 0$
\State $m = q - d + 1$ 
\State $k = \left[0,1,...,1\right]$   \Comment{vector of length d}
\State $\hat{k} = \left[m,m,...,m\right]$  \Comment{vector of length d}
\Repeat 
   \State $k(p) = k(p) + 1$
   \If {$k(p) > \hat{k}(p)$}
      \If {p=d}
         \State All indices enumerated!
      \Else
         \State $k(p) = 1$
         \State $p = p + 1$ 
      \EndIf
   \Else
      \For {$j=0:p$}
         \State $\hat{k}(j) = \hat{k}(p) - k(p) + 1$
      \EndFor
      \State $k(0) = \hat{k}(0)$
      \State $p = 1$
      \State Return valid index set k!
   \EndIf
\Until{$k = \left[0,...,0\right]$}
\end{algorithmic}
\end{algorithm}

With all the index sets for some Smolyak level $q$ available through the code segment in \ref{code:enumeration1}, the tensor product appearing in \ref{eq:Smolyak_formula2} can be evaluated with the aid of an additional enumeration routine. Each indice in an index set $\left\{i_1+i_2+...+i_d\right\}$ corresponds to certain number of knots, which for example, is given by \ref{eq:cc_numpoints} for Clenshaw-Curtis. All the components in the tensor product can be given by the following enumeration algorithm, which is based on the algorithm in \cite{Holtz}. Input to algorithm \ref{code:enumeration2} should be based on output from algorithm \ref{code:enumeration1}. Specifically for some index set $\left\{i_1,i_2,...,i_d\right\}$ returned by algorithm \ref{code:enumeration1}, each indice should be converted to a corresponding number of knots and input to algorithm \ref{code:enumeration2}.     
\begin{algorithm}
\caption{\label{code:enumeration2} 
Code for enumerating all components of a tensor product. The input is a $d$ dimensional vector $m$ where each entry $m_j$ corresponds to the number of knots in a collocation scheme of level $i_j$.} 
\begin{algorithmic}[1]
\State $p = 0$
\State $s = \left[0,1,1,...,1\right]$  \Comment{vector of length d}
\Repeat 
   \State $s(p) = s(p) + 1$
   \If{$s(p) > m(p)$}
      \If{$p = d-1$}
         \State All indices enumerated!
      \Else
         \State $s(p) = 1$
         \State $p = p + 1$
      \EndIf
   \Else
      \State $p = 0$
      \State Return valid enumeration set!
   \EndIf
\Until{$s = m$} 
\end{algorithmic}
\end{algorithm}

\subsubsection{Sampling Sparse Grid Interpolant of Correlated Variables} \label{subsubsec:sampling_smolyak}

The reduced order model in \ref{eq:decomp_smol3} consists of linear coombinations of Smolyak interpolants. Evaluation of the reduced order model is equivalent to finding the value at several Smolyak interpolants and summing the results. However, recall that in the Smolyak algorithm for building the interpolants described in \ref{subsec:algorithm_mechanics} each dimension in the sparse grid is built orthogonal to the others. Implicit in this construction is the assumption that the random variables comprising the objective function are independent. In fact, in many computer codes the random variables forming the parameter space are correlated. The degree of correlation among the random variables is generally described using a covariance matrix. 

With the availability of a covariance matrix for the input random variables it is possible to sample the reduced order model hundreds or thousands of times to get accurate and precise statistical moments. Evaluating a Smolyak interpolant is relatively cheap and fast so this is not a computational problem. To produce correlated  random variables from a Gaussian population the Kaiser-Dichman method can be applied \cite{KaiserDichman}. Say some inputs to a computer code are normally distributed with mean $\mu$ and covariance $\Sigma$. To produce a random sample $\mu'$ from $\Sigma$ one can apply,
\begin{equation} \label{eq:sample_covariance}
   \mu' = \mu + U^T\pi
\end{equation} 
where $U^T$ is the lower triangular matrix arising from the Cholesky decomposition of $\Sigma$ and $\pi$ is a standard normal random vector. 

The Cholesky decomposition of a covariance matrix $C=U^{T}U$ is the equivalent to taking the square root of a matrix. A Cholesky matrix transform, or left multiplication by $U^T$, maps uncorrelated variables into correlated variables with covariance matrix $\Sigma$. Consequently, in \ref{eq:sample_covariance} $\mu'$ is normally distributed with mean $\mu$ and covariance $\Sigma$. A statistically significant number of instances of $\mu'$ should be calculated and evaluated at the reduced order model. Even though the model is built assuming independent variables this method takes into account any correlations when calculating statistical moments. When evaluating any Smolyak interpolant at a number of sampled points one must always make sure all the sampled points lay inside the bounds of the hypercube forming the sparse grid.

\subsubsection{Dimension Truncation} \label{subsubsec:dimension_truncation}

By definition a reduced order model contains less dimensions than the original model whose reduction is intended. Consequently, a methodology for identifying important dimensions is necessary. For the purposes of this thesis, the importance of a random input variable on some objective function is determined by its contribution to the function's variance. While there are methods for exactly calculating the truncation and superposition dimensions of a function \cite{Holtz}, these methods require the computation of all $2^d$ component functions in an \ac{ANOVA} decomposition. For typical computer codes used in engineering this requirement is not feasible and so the effective dimensions of a function must be estimated adaptively. 

To this end, construction of a reduced order model based on the anchored-\ac{ANOVA} decomposition begins with calculation of the zeroth-order and all first-order components. The zeroth-order component function is simply the function evaluated at the anchor point,
\begin{equation} \label{eq:zeroth_component}
   f_{\left\{\emptyset\right\}} = f(\overline{\textbf{x}}).  
\end{equation}  
From the recursive definition of the anchored-\ac{ANOVA} decomposition in \ref{eq:recurvsive_component_def}, the $i^{\text{th}}$ component function is given as,
\begin{equation} \label{eq:first_component}
   f_{\left\{i\right\}} = f(\textbf{x})\vert_{
    \textbf{x}\setminus x_i} - f_{\left\{\emptyset\right\}}. 
\end{equation} 
The first order components solely measure the affect of the $i^{\text{th}}$ random variable on the function output. Therefore, for each of $d$ random variables contributing to a function's variability a sensitivity coefficient can be calculated as \cite{AHSGC_HighDimensions},
\begin{equation} \label{eq:anova_sensitivity}
   \eta_i = \frac{
    \int_{\Omega_i}
     \left[f(\textbf{x})\vert_{
     \textbf{x}=\overline{\textbf{x}}\setminus x_i}
      -f(\textbf{x}) \right]\rho(x_i)dx_i}
       {
       f_{\left\{\emptyset\right\}}}
\end{equation}
where $\rho(x_i)$ is the probability density of $x_i$. If $f$ is a function of spatial coordinates then the $L_2$ norm can be applied to \ref{eq:anova_sensitivity}. The greater the affect of the $i^{\text{th}}$ random variable on the function output, the greater the sensitivity coefficient.

Once all the first order anchored-\ac{ANOVA} components are calculated the sensitivity coefficients in \ref{eq:anova_sensitivity} can be used to identify the important dimensions. To start, each $\eta_i$ should be normalized by the sum of all $\eta_i$. The important dimensions can then be obtained by taking all $i$ variables such that $\eta_i > \theta_1$. A value of $\theta_1 = .02$ has been shown to be effective for many problems \cite{Hesthaven_ANOVA}. At the very least, the reduced order model described here will consists of the zeroth order and all first order anchored-\ac{ANOVA} components, giving it a superposition dimension of one. The variance and mean of the reduced order model with superposition dimension one should be obtained through sampling using \ref{eq:sample_covariance}.  

To improve accuracy higher order components can be included. Higher order components should only be built using combinations of the dimensions deemed important. If all dimensions were to be included construction of a reduced order model based on the \ac{ANOVA} decomposition would quickly become intractable for most engineering problems. It can be argued that if a set of random variables independently affect a function then their interactions will likely also affect the function. Say all first order components have been constructed and the important dimensions have been added to a set $\mathcal{T}$. The next step in creating a higher order model is to construct anchored-\ac{ANOVA} components utilizing dimensions $t$ such that $t\subset\mathcal{T}$ and $\vert t\vert=2$. In total there should be $C(\vert\mathcal{T}\vert,2)$ such components. In general, the number of $p$ order components can be given as,
\begin{equation} \label{eq:num_components_rom_order_p}
   C(\vert\mathcal{T}\vert,p) = \frac{\vert\mathcal{T}\vert!}{
    (p!)(\vert\mathcal{T}\vert-p)!}.
\end{equation}
To determine whether additional orders need to be included in the reduced order model, the mean of the current reduced order model should be compared to the mean of the previous order model. If the relative change in mean doesn't exceed some threshold $\theta_2$ then the reduced order model is considered converged. Otherwise, higher order components should be added. In this manner, a reduced order model for a computer code can be constructed adaptively.    

\subsection{Algorithm for Building a Reduced Order Model} \label{subsec:full_algorithm}

%Low order smolyak grids needed is has been shown. First order should be good most of time...especially in nuclear applications where adjoints have been so successful.          