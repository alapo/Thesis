\section{Overview of Previous Work} 
\label{sec:literature_review}

Modeling nuclear reactor stability and performance computationally has evolved into a multi-physics and multi-scale regime. Various computer codes have been developed and optimized to model individual facets of reactor operation such as neutronics, thermal-hydraulics, and kinetics. These codes are most often coupled to produce more physical results. While it is crucial to be able to produce best-estimate calculations for the design and safety analysis of nuclear reactors, it is equally important to obtain design margins by propagating uncertainty information through the entire computational process. Replacing seemingly arbitrary design margins based on engineering judgement with design margins based on the uncertainty quantification of computer codes can help overcome significant human shortcomings \cite{Eardley}.   

The methodologies used for uncertainty quantification in nuclear engineering have generally mirrored the computational resources available during the time of development. When computational resources were relatively limited, methods based on perturbation theory were derived since the method computation times are independent of the number of input parameters \cite{MLWilliams}. Perturbation theory applies a number of linear approximations and adjoint operators to allow for the retrieval of sensitivity coefficients and basic statistical moments of responses of interest. However, in perturbation theory each desired response requires the solution of a deterministic equation and in some inhomogeneous problems in reactor physics the linear approximations may fail \cite{Gandini}. In addition, application of perturbation theory techniques to computer codes is highly intrusive. Coupled, multiphysics uncertainty analyses using perturbation theory are prohibitive especially when legacy codes are involved. Yet another drawback of perturbation theory in neutronics applications arises in the adjoint-based formulation for cross section uncertainty propagation. In this case, exceptions must be made to responses that cannot be expressed as ratios of reaction rates, as in the transport cross section and \ac{ADFs} \cite{TwoStep_Approach} \cite{Yankov1}. 

As computer resources increased, the linear approximations of perturbation theory were gradually relaxed until routine Monte Carlo sampling became feasible. In the XSUSA method a 44-group covariance matrix is stochastically sampled to generate perturbed few-group cross sections that can be used as input to a Monte Carlo uncertainty analysis in a core simulator \cite{Klein_Gallner}. While uncertainty quantification using Monte Carlo sampling does not apply any approximations to the mathematical system under investigation, the computational expense is tremendous, and the extraction of information such as sensitivity coefficients can be difficult. Nevertheless, sampling provides a brute force means by which many uncertainty quantification algorithms can be validated. 

Recently, an approach based on the stochastic finite element method, referred to as polynomial chaos expansion, has been developed for the purposes of uncertainty quantification \cite{Ghanem_Spanos}. The basic idea behind polynomial chaos expansions is to expand the random variables of some governing stochastic partial differential equations in terms of orthogonal polynomials and then to apply Galerkin projections \cite{LeMaitreKnio}. This idea is similar to the angular expansion of the scatter cross section in terms of Legendre polynomials in neutron transport theory. Methods for uncertainty quantification that employ polynomial chaos expansions fall under the general mathematical framework of spectral methods. The motivation behind spectral methods for uncertainty quantification is to combine the rapid convergence rates of deterministic methods with the generality of stochastic sampling methods. 

While used extensively in structural and materials engineering, polynomial chaos expansions have been seldom applied in nuclear engineering. In \cite{MMR_Williams1} and \cite{MMR_Williams2} the author applies analytic polynomial chaos techniques to investigate the effect of random material properties on radiation transport through a slab in the P1 approximation. The authors in \cite{Ayres_Williams_Eaton} apply non-linear polynomial chaos to examine static and dynamic eigenvalue uncertainties due to uncertainties in cross section values. Finally, the authors in \cite{Fichtl_Prinja} use polynomial chaos-collocation to study the effects of total cross section uncertainties on the probability density function of the scalar flux in absorbing and diffusive media.    

The polynomial chaos expansions based on stochastic galerkin methods applied in the research above exhibit several substantial advantages over Monte Carlo and perturbative methods. Mainly, spectral convergence rates can be achieved. In addition, while nonlinearities and large uncertainties in random variables pose problems for perturbation methods, the stochastic galerkin methods easily deal with such factors. However, the primary drawback of polynomial chaos expansions with stochastic galerkin methods lies in their intrusive nature. In other words, unless an engineering code is initially developed with stochastic galerkin capabilities, which is unlikely, extensive modifications will have to be made to the code. Modifications include adding the ability to solve an enlarged system of coupled, partial differential equations.      

To this end the stochastic collocation method, a variant of polynomial chaos expansion, is considered since it circumvents the need to modify engineering code in order to implement spectrally converging uncertainty quantification methods. Stochastic collocation methods are essentially interpolatory, projecting a set of deterministic simulations onto a polynomial basis. With stochastic collocation codes can be treated as "black boxes". Such an attribute is highly desireable for the uncertainty analysis of nuclear engineering computer codes since often these consist of coupled, multi-physics, and multi-scale legacy code. The success of perturbatory methods, which are linear, in the uncertainty analysis of nuclear engineering codes is indicative of the potential for stochastic collocation. Being a spectral method, stochastic collocation performs best on smooth functions \cite{BoydSpec}.    

However, even when utilizing a stochastic collocation method based on sparse grid constructs, the method suffers from the so-called "curse of dimensionality" \cite{LeMaitreKnio}. The "curse of dimensionality" arises when the convergence of a method is proportional to the exponent of the the function's dimensionality, as in a full tensor product interpolation scheme. While sparse grids help to mitigate the "curse of dimensionality" researchers have shown that when applied to realistic engineering computer simulations the practicality of stochastic collocation diminishes for dimensions greater than $\mathcal{O}(10)$ \cite{AHSGC_HighDimensions}. Consequently, further mitigation is needed in order to apply stochastic collocation to real-world engineering problems. To this end, reduced order models, also known as surrogates and response surfaces, are utilized. Reduced order models work to find a relatively small subspace of some function that is still representative of the original function space. 

Performing uncertainty quantification on high-dimensional, coupled, multiphysics engineering codes can be prohibitive if the codes take many hours to run. To make uncertainty quantification feasible for such problems reduced order models are constructed. While a reduced order model may be computationally expensive to build, the ideal end product is a function that can be evaluated in seconds that captures the behavior of the objective function on which the reduced order model is based.    

%Time to build rom at first, but then pays off for UQ, optimization, inference.
%consequently, reduced order model/surrogate is needed. mathematical surrogate
% discuss paper
% Uncertainty quantification using reduced order models. Stochastic collocation curse of dimensionality, not practical for high dimensions. Must use dimension reduction to make parctical.


